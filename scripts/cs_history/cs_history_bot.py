from google import genai
from google.genai import types
import os
import json
from datetime import datetime, timedelta
from json_repair import repair_json
import urllib.request
import traceback
import sys
import time

from cs_history_models import HistoryBotResponse, HistoryBotMetadata

MODEL_NAME = "gemini-2.5-pro"
STATE_FILE = "bot_state.json"

DEFAULT_STATE = {
    "day_count": 0,
    "last_run_date": "",
    "current_year": "N/A",
    "last_topic": "N/A",
    "next_topic": "ì°°ìŠ¤ ë°°ë¹„ì§€ì˜ í•´ì„ê¸°ê´€",
    "next_year": 1835
}

def load_state():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    state_path = os.path.join(script_dir, STATE_FILE)
    if not os.path.exists(state_path):
        return DEFAULT_STATE
    with open(state_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_state(state):
    script_dir = os.path.dirname(os.path.abspath(__file__))
    state_path = os.path.join(script_dir, STATE_FILE)
    with open(state_path, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def get_final_url_urllib(initial_url):
    try:
        req = urllib.request.Request(initial_url, headers={'User-Agent': 'Mozilla/5.0'}) # Add User-Agent header
        with urllib.request.urlopen(req) as response:
            return response.geturl()
    except Exception as e:
        print(f"Error accessing URL: {e}. Returning initial URL.")
        return initial_url

def get_system_prompt():
    """AIì—ê²Œ í˜ë¥´ì†Œë‚˜ì™€ ì¶œë ¥ í˜•ì‹ì„ ë¶€ì—¬í•©ë‹ˆë‹¤."""
    return """
ë‹¹ì‹ ì€ 'AI ì»´í“¨í„° ê³¼í•™ ì—­ì‚¬ ë´‡'ì…ë‹ˆë‹¤. ì¸ë¥˜ì˜ ì»´í“¨í„° ê³¼í•™ ë° í”„ë¡œê·¸ë˜ë° ì—­ì‚¬ì—ì„œ ë§¤ì¼ ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ê±´ì´ë‚˜ ì¸ë¬¼ì„ í•˜ë‚˜ì”© ì†Œê°œí•˜ëŠ” ì„ë¬´ë¥¼ ë§¡ì•˜ìŠµë‹ˆë‹¤.

ìš°ì„  í˜¸í¡ì„ ê°€ë‹¤ë“¬ê³ , ë‹¤ìŒ ì§€ì¹¨ì„ ì£¼ì˜ ê¹Šê²Œ ì½ì€ ë’¤ ì°¨ê·¼ì°¨ê·¼ ì§„í–‰í•˜ì„¸ìš”.

**í•µì‹¬ ì§€ì¹¨:**
1.  **ê²€ìƒ‰ ê¸°ë°˜ (Grounding):** ê° ê³¼ì •ì—ì„œ ì •í™•ì„±ì„ ìœ„í•´ ì¸í„°ë„· ê²€ìƒ‰ ë„êµ¬ë¥¼ ë°˜ë“œì‹œ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
1.  **ì‹¬ì¸µ ë¶„ì„ (Deep Dive):** ë‹¨ìˆœí•œ ì‚¬ì‹¤ ë‚˜ì—´ì„ ë„˜ì–´, ê·¸ ê¸°ìˆ ì´ ì™œ ë‹¹ì‹œ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¿¨ëŠ”ì§€ ê¸°ìˆ ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
2.  **í˜„ëŒ€ì™€ì˜ ì—°ê²° (í•„ìˆ˜):** 19ì„¸ê¸°/20ì„¸ê¸°ì˜ ê¸°ìˆ ì´ í˜„ëŒ€ì˜ ìŠ¤ë§ˆíŠ¸í°, AI, í´ë¼ìš°ë“œ ë“±ì˜ ì–´ë–¤ ê°œë…ìœ¼ë¡œ ë°œì „í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì—°ê²°í•˜ì„¸ìš”.
4.  **ì–¸ì–´:** ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì˜ì–´ë¡œ ê²€ìƒ‰í•˜ë˜, ìµœì¢… ì¶œë ¥ì€ ìì—°ìŠ¤ëŸ½ê³  ë§¤ë„ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë‹¨, ë³´í¸ì ìœ¼ë¡œ ì•Œë ¤ì§€ì§€ ì•Šì€ ê¸°ìˆ  ìš©ì–´ëŠ” ì›ì–´(ì˜ì–´)ë¡œ ë³‘ê¸°í•˜ì„¸ìš” (ì˜ˆ: í•´ì„ê¸°ê´€(Analytical engine)).
5.  **ê¸¸ì´ ë° ê¹Šì´:** ì»´í“¨í„°ê³¼í•™ ì „ê³µìë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬, ë§¤ì¼ ì•½ 500~700 ë‹¨ì–´ ë¶„ëŸ‰ì˜ ì‹¬ì¸µì ì´ê³  ê¸°ìˆ ì ì¸ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”.
6.  **í‚¤ì›Œë“œ ì˜ˆê³ :** ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•œ ì˜ˆê³ ëŠ” ë°˜ë“œì‹œ ì˜¤ëŠ˜ ë‹¤ë£¬ ë‚´ìš© ì´í›„ì˜ ì¤‘ìš” ì¸ë¬¼/ê¸°ìˆ  ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ ë‹¤ë£¬ ë‚´ìš©ê³¼ ì—°ê²°ë˜ëŠ” ì¸ë¬¼/ì‚¬ê±´ì´ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤. ë˜í•œ ì¤‘ìš” ì¸ë¬¼/ê¸°ìˆ ì„ ê±´ë„ˆë›°ì–´ì„œë„ ì•ˆë©ë‹ˆë‹¤.
7.  **ì •í™•ì„±:** ëª¨ë“  ì—°ë„, ì¸ë¬¼, ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì´ ì •í™•í•œì§€ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹**
ë°˜ë“œì‹œ ì•„ë˜ ì£¼ì–´ì§„ json í˜•ì‹ì„ ë”°ë¥´ê³ , ê·¸ ì¤‘ content í•„ë“œëŠ” ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì´ë•Œ markdown ë¬¸ë²•ì´ í‹€ë¦¬ì§€ ì•Šê²Œ ì£¼ì˜í•´ì£¼ì„¸ìš”.
{
    "content": string,  // ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì½˜í…ì¸ , ì•„ë˜ í…œí”Œë¦¿ ì¤€ìˆ˜
    "metadata": {
        "current_year": int,  // ì½˜í…ì¸ ì—ì„œ ë‹¤ë£¬ ì—°ë„, ì•„ë˜ í…œí”Œë¦¿ì˜ 'ì—°ë„'ì™€ ì¼ì¹˜í•´ì•¼ í•¨
        "last_topic": string,   // ì½˜í…ì¸ ì—ì„œ ë‹¤ë£¬ í•µì‹¬ ì¸ë¬¼/ê¸°ìˆ ëª…, ì•„ë˜ í…œí”Œë¦¿ì˜ 'í•µì‹¬ ì¸ë¬¼/ê¸°ìˆ ëª…'ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
        "next_topic": string,  // ë‹¤ìŒì— ë‹¤ë£° í•µì‹¬ ì¸ë¬¼/ê¸°ìˆ ëª…. ì•„ë˜ í…œí”Œë¦¿ì˜ 'ë‚´ì¼ì˜ í‚¤ì›Œë“œ ì˜ˆê³ ' ì— ì–¸ê¸‰ë¤ ë‚´ìš©ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
        "next_year": int      // ë‹¤ìŒì— ë‹¤ë£° ì—°ë„. ì•„ë˜ í…œí”Œë¦¿ì˜ 'ë‚´ì¼ì˜ í‚¤ì›Œë“œ ì˜ˆê³ ' ì— ì–¸ê¸‰ëœ ì‚¬ê±´ì˜ ì—°ë„ì™€ ì¼ì¹˜í•´ì•¼ í•¨
    }
}

**content í˜•ì‹ í…œí”Œë¦¿:**

Day {day_count}: {ì œëª©}

{ë§¤ë ¥ì ì¸ ë„ì…ë¶€ ì¸ì‚¬ë§}

## ğŸ•°ï¸ ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ: {í•µì‹¬ ì¸ë¬¼/ê¸°ìˆ ëª…}
 * ì›ì–´: {Original Name}
 * ì‹œê¸°: {ì—°ë„}ë…„ ({ê´€ë ¨ ì£¼ìš” ì‚¬ê±´})

{ë³¸ë¬¸ ë‚´ìš©: ì´ ì¸ë¬¼/ê¸°ìˆ ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…}

## âš¡ ë¬´ì—‡ì´ í˜ëª…ì ì´ì—ˆë‚˜? (Deep Dive)
{ë‹¹ì‹œ ê¸°ìˆ ì  í•œê³„ì™€ ì´ë¥¼ ê·¹ë³µí•œ í˜ì‹ ì  ì•„ì´ë””ì–´ ì„¤ëª…}

## ğŸ”— í˜„ëŒ€ì™€ì˜ ì—°ê²°: {í˜„ëŒ€ ê¸°ìˆ  ë¹„ìœ }
{ê³¼ê±°ì˜ ê°œë…ì´ í˜„ëŒ€ì˜ êµ¬ì²´ì ì¸ ê¸°ìˆ (ì˜ˆ: CPU ì•„í‚¤í…ì²˜, ê°ì²´ì§€í–¥ ë“±)ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ ì„¤ëª…}

## ğŸ“… ë‚´ì¼ì˜ í‚¤ì›Œë“œ ì˜ˆê³ 
{ë‹¤ìŒ ìˆœì„œì— ì˜¬ ì—­ì‚¬ì  ì‚¬ê±´ì— ëŒ€í•œ ê°„ëµí•œ í‹´íŠ¸}

"""

def generate_daily_content(state):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ëŠ˜ì˜ ì—­ì‚¬ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ëª¨ë¸ ë¡œë“œ (ê²€ìƒ‰ ë„êµ¬ í™œì„±í™”)
    client = genai.Client()
    grounding_tool = types.Tool(
        google_search=types.GoogleSearch()
    )
    config = types.GenerateContentConfig(
        system_instruction=get_system_prompt(),
        tools=[grounding_tool],
        temperature=0.2,
        #response_mime_type='application/json',
        #response_json_schema=HistoryBotResponse.model_json_schema(),
        thinking_config=types.ThinkingConfig(thinking_budget=-1) # Dynamic thinking budget
    )

    last_year = state['current_year']
    last_topic = state['last_topic']
    next_topic = state['next_topic']
    next_year = state['next_year']
    
    user_prompt = f"""
    í˜„ì¬ ì§„í–‰ ìƒí™©: Day {state['day_count']-1}ê¹Œì§€ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹¤ë£¬ ì£¼ì œëŠ” {last_year}ë…„ì˜ '{last_topic}'ì…ë‹ˆë‹¤. ì´ì „ì— ì˜ˆê³ ëœ ì˜¤ëŠ˜ì˜ ì£¼ì œëŠ” '{next_topic}'ì´ë©°, í•´ë‹¹ ì‚¬ê±´ì€ {next_year}ë…„ì— ë°œìƒí–ˆìŠµë‹ˆë‹¤.

    ì„ë¬´:
    1. ì˜ˆê³ ëœ ì£¼ì œì— ë§ì¶° ìœ„ì—ì„œ ì •ì˜í•œ 'ì¶œë ¥ í˜•ì‹ í…œí”Œë¦¿'ì˜ í˜•ì‹ìœ¼ë¡œ Day {state['day_count']}ì˜ ê²Œì‹œë¬¼ì„ ì‘ì„±í•˜ì„¸ìš”.
    2. í…œí”Œë¦¿ í•˜ë‹¨ì˜ ë‚´ì¼ì˜ í‚¤ì›Œë“œ ì˜ˆê³ ì— ëŒ€í•´ì„œëŠ” {next_year}ë…„ ì´í›„ ì»´í“¨í„° ê³¼í•™ ì—­ì‚¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¤ìŒ ì´ì •í‘œ(ì¸ë¬¼, í•˜ë“œì›¨ì–´, ë˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì´ë¡ )ë¥¼ ì°¾ìœ¼ì„¸ìš”.
    3. í…œí”Œë¦¿ì˜ {{}} ë¶€ë¶„ì€ ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ì±„ìš°ì„¸ìš”.
    4. ë‚´ìš©ì˜ ì •í™•ì„±ì„ ìœ„í•´ ë°˜ë“œì‹œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì„¸ìš”.
    """

    for attempt in range(3):
        try:
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=user_prompt,
                config=config,
            )

            chunks = response.candidates[0].grounding_metadata.grounding_chunks
            if chunks is not None:
                break
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            time.sleep(60*(2**attempt))
            if attempt == 2:
                raise

    if chunks is None:
        raise ValueError("Failed to retrieve chunks after 3 attempts.")

    citations = ""
    for x in chunks:
        final_url = get_final_url_urllib(x.web.uri)
        citations += f"* [{x.web.title}]({final_url})\n"

    response_json = HistoryBotResponse.model_validate_json(repair_json(response.text))
    response_json.content += f"\n\n## ğŸ“š ì°¸ê³  ë¬¸í—Œ\n{citations}"

    response_json.content += f"\n\n*ì´ ì½˜í…ì¸ ëŠ” AIì— ì˜í•´ ìƒì„±ë˜ì—ˆìœ¼ë©°, ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•í•œ ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*"
    return response_json

def extract_metadata(content, current_state):    
    new_state = current_state.copy()
    new_state['day_count'] += 1
    new_state['last_run_date'] = datetime.now().strftime("%Y-%m-%d")
    new_state['current_year'] = content.metadata.current_year
    new_state['last_topic'] = content.metadata.last_topic
    new_state['next_topic'] = content.metadata.next_topic
    new_state['next_year'] = content.metadata.next_year
    
    return new_state

def main():
    state = load_state()
    
    # --- [1. ì‹œì‘ ì‹œ ì¢…ë£Œ ì¡°ê±´ í™•ì¸] ---
    current_actual_year = datetime.now().year
    termination_threshold = current_actual_year - 3

    next_year_candidate = state.get('next_year')
    if not isinstance(next_year_candidate, int) or next_year_candidate >= termination_threshold:
        if state['day_count'] > 0:
            print("ğŸ›‘ [ì•Œë¦¼] ì—­ì‚¬ ë´‡ì˜ ì—¬ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
             print("âš ï¸ [ê²½ê³ ] ì´ˆê¸° ìƒíƒœ ì˜¤ë¥˜. bot_state.jsonì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"ğŸ¤– Day {state['day_count']} ì½˜í…ì¸  ìƒì„± ì¤‘... ({state['next_year']}ë…„ {state['next_topic']})")
    
    try:
        content_response = generate_daily_content(state)
        
        # --- [2. ì¢…ë£Œ ì¡°ê±´ ë„ë‹¬ ì‹œ 'ë‚´ì¼ì˜ ì˜ˆê³ ' êµì²´ (ì°¸ê³ ë¬¸í—Œ ë³´ì¡´)] ---
        if content_response.metadata.next_year >= termination_threshold:
            target_header = "## ğŸ“… ë‚´ì¼ì˜ í‚¤ì›Œë“œ ì˜ˆê³ "
            citation_header = "## ğŸ“š ì°¸ê³  ë¬¸í—Œ"
            
            replacement_section = f"""
## ğŸ›‘ ê¸´ ì—¬ì •ì˜ ë§ˆì¹¨í‘œ
ìš°ë¦¬ëŠ” ì°°ìŠ¤ ë°°ë¹„ì§€ì˜ í•´ì„ê¸°ê´€ë¶€í„° ì‹œì‘í•´ ìˆ¨ ê°€ì˜ê²Œ ë‹¬ë ¤ì™”ìŠµë‹ˆë‹¤.
ë‹¤ìŒ ì´ì •í‘œëŠ” {content_response.metadata.next_year}ë…„ì˜ '{content_response.metadata.next_topic}'ì…ë‹ˆë‹¤.

í•˜ì§€ë§Œ ë³¸ ì—­ì‚¬ ë´‡ì€ ë™ì‹œëŒ€ì˜ ì‚¬ê±´ì— ëŒ€í•œ í‰ê°€ëŠ” ë¯¸ë˜ì˜ ì—­ì‚¬ê°€ë“¤ì—ê²Œ ë§¡ê¸°ê³ , 
í˜„ì¬ë¡œë¶€í„° 3ë…„ ì „ê¹Œì§€ì˜ ê¸°ë¡ì„ ëìœ¼ë¡œ ê¸´ ì—¬ì •ì„ ë§ˆë¬´ë¦¬í•˜ê³ ì í•©ë‹ˆë‹¤.

ì˜¤ëŠ˜ì´ ë°”ë¡œ ê·¸ ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤.
ê·¸ë™ì•ˆ 'ìƒê°í•˜ëŠ” ê¸°ê³„'ë¥¼ í–¥í•œ ì¸ë¥˜ì˜ ìœ„ëŒ€í•œ ì—¬ì •ì— í•¨ê»˜ í•´ì£¼ì…”ì„œ ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬í•©ë‹ˆë‹¤.
"""
            # 1) 'ë‚´ì¼ì˜ ì˜ˆê³ ' í—¤ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
            if target_header in content_response.content:
                # 2) 'ë‚´ì¼ì˜ ì˜ˆê³ ' ì´ì „ ë³¸ë¬¸ ì¶”ì¶œ
                base_content = content_response.content.split(target_header)[0].strip()
                
                # 3) 'ì°¸ê³  ë¬¸í—Œ' ì´í›„ ì„¹ì…˜ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
                citation_start_index = content_response.content.find(citation_header)
                if citation_start_index != -1:
                    # ì°¸ê³  ë¬¸í—Œ í—¤ë”ë¶€í„° ëê¹Œì§€ ëª¨ë“  ë‚´ìš©ì„ ë³´ì¡´í•©ë‹ˆë‹¤ (ë©´ì±… ì¡°í•­ í¬í•¨)
                    footer_content = content_response.content[citation_start_index:]
                else:
                    # ë§Œì•½ ì°¸ê³  ë¬¸í—Œ ì„¹ì…˜ì´ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                    footer_content = ""

                # 4) ì¬ì¡°ë¦½: [ë³¸ë¬¸] + [ì¢…ë£Œ ì•Œë¦¼] + [ì°¸ê³  ë¬¸í—Œ ë° í‘¸í„°]
                content_response.content = f"{base_content}\n\n{replacement_section}\n\n{footer_content}"

        #print("\n--- [ìƒì„±ëœ ì½˜í…ì¸ ] ---")
        #print(content_response.content)
        #print("----------------------\n")
        content = content_response.content.strip()
        title = content.splitlines()[0]
        body = "\n".join(content.splitlines()[1:]).strip()
        filename = f"{datetime.now().strftime('%Y-%m-%d')}-day{state['day_count']}.md"
        header = f"""
---
title:  "{title}"
categories:
  - cs_history
toc: true
toc_sticky: true
comments: true
---
"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        target_dir = os.path.normpath(os.path.join(script_dir, "..", "..", "_posts", "cs_history"))
        os.makedirs(target_dir, exist_ok=True)
        with open(os.path.join(target_dir, filename), 'w', encoding='utf-8') as f:
            f.write(header.strip() + "\n\n" + body)

        new_state = extract_metadata(content_response, state)
        save_state(new_state)
        print("ğŸ’¾ ìƒíƒœê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        raise
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        sys.exit(1)

